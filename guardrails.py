import datetime
import argparse
import os
import json
import shutil

from pathlib import Path
from dotenv import load_dotenv
import google.generativeai as genai
from google.ai.generativelanguage_v1beta.types import content
import ollama


from src.utils.dirs import remove_empty_dirs
from src.utils.pdf import pdf_to_image

load_dotenv()
gemini_api_key = os.getenv("GEMINI_API_KEY")

response_schema = content.Schema(
    type=content.Type.OBJECT,
    enum=[],
    required=["analysis", "has_sensitive_data", "leaked_fields"],
    properties={
        "analysis": content.Schema(
            type=content.Type.STRING,
            description="Explica√ß√£o passo a passo. Se houver vazamento, descreva onde e o que foi lido.",
        ),
        "has_sensitive_data": content.Schema(
            type=content.Type.BOOLEAN,
            description="TRUE se houver qualquer PII (nome, cpf, conta) leg√≠vel. FALSE se tudo estiver censurado/mascarado.",
        ),
        "leaked_fields": content.Schema(
            type=content.Type.ARRAY,
            description="Lista dos tipos de dados que vazaram (ex: ['NOME_DO_PAGADOR', 'CPF_DESTINATARIO']). Retorne lista vazia [] se tudo estiver seguro.",
            items=content.Schema(type=content.Type.STRING),
        ),
    },
)

generation_config = {
    "temperature": 0.0,
    "top_p": 1.0,
    "top_k": 1,
    "max_output_tokens": 2048,
    "response_mime_type": "application/json",
    "response_schema": response_schema,
}

genai.configure(api_key=gemini_api_key)
gemini_client = genai.GenerativeModel(
    model_name="gemini-2.5-flash", generation_config=generation_config
)

prompt = """
<PERSONA>
Voc√™ √© um Auditor de Seguran√ßa da Informa√ß√£o (DLP) altamente c√©tico. Sua √∫nica fun√ß√£o √© bloquear o vazamento de PII (Personally Identifiable Information).
</PERSONA>

<CONTEXTO_VISUAL>
Voc√™ est√° analisando comprovantes banc√°rios Pix.
- Estrutura t√≠pica: Um R√ìTULO (ex: "Destinat√°rio") seguido de um VALOR (ex: "Jo√£o da Silva").
- O usu√°rio tentou anonimizar os VALORES aplicando tarjas pretas (ret√¢ngulos s√≥lidos).
</CONTEXTO_VISUAL>

<DEFINICAO_DE_DADO_SENSIVEL>
Considere como SENS√çVEL (Vazamento) se qualquer um destes estiver vis√≠vel:
1. Nomes de Pessoas (Pessoa F√≠sica). Nota: Nomes de Bancos ou Institui√ß√µes de Pagamento N√ÉO s√£o sens√≠veis.
2. CPF ou CNPJ (parcial ou total, observe que se o CNPJ for da institui√ß√£o que est√° enviando ou recebendo o Pix n√£o √© um dado sens√≠vel, apenas se for chave Pix).
3. Ag√™ncia e Conta.
4. Chaves Pix (E-mail, Telefone, CPF).
</DEFINICAO_DE_DADO_SENSIVEL>

<REGRAS_DE_OURO>
1. **R√≥tulo != Valor:** O texto "CPF" ou "Nome" impresso no layout √© apenas um r√≥tulo. Isso √© SEGURO. O vazamento s√≥ ocorre se o N√öMERO do CPF ou o NOME da pessoa estiver leg√≠vel.
2. **A Regra da Tarja:** - Se um valor est√° coberto por uma tarja preta s√≥lida: SEGURO (Ignore).
   - Se a tarja √© transl√∫cida e permite leitura: VAZAMENTO.
   - Se a tarja cobre apenas metade do nome/n√∫mero: VAZAMENTO.
3. **Ignore (Safe List):**
   - Valores monet√°rios (R$ 50,00).
   - Datas e Hor√°rios.
   - IDs de Transa√ß√£o (sequ√™ncias longas de letras e n√∫meros aleat√≥rios).
   - Nomes de Bancos (ex: "Nubank", "Ita√∫", "Banco Central").
   - Mensagens de rodap√©.
</REGRAS_DE_OURO>

<PROCEDIMENTO_DE_ANALISE>
Analise cada campo visualmente:
Passo 1: Identifique um campo (ex: Nome do Favorecido).
Passo 2: Olhe para o valor deste campo.
Passo 3: O valor √© leg√≠vel? 
    - N√ÉO (tem tarja preta) -> OK.
    - SIM -> √â um nome de banco ou dado da Safe List?
        - SIM -> OK.
        - N√ÉO -> ALERTA DE VAZAMENTO.
</PROCEDIMENTO_DE_ANALISE>

<FORMATO_RESPOSTA>
Retorne apenas o JSON. Se encontrar vazamento, adicione o nome do campo em `leaked_fields`.
"""


def check_sensitive_data_ollama(file_path):
    temp_image = None
    try:
        file_ext = Path(file_path).suffix.lower()
        if file_ext == ".pdf":
            temp_image = pdf_to_image(file_path)
            file_path = temp_image

        response = ollama.chat(
            model="qwen2.5vl:7b",
            messages=[
                {
                    "role": "user",
                    "content": prompt,
                    "images": [file_path],
                }
            ],
            options={"temperature": 0.0},
            format="json",
        )

        response_content = response["message"]["content"]

        try:
            result = json.loads(response_content)
        except json.JSONDecodeError:
            import re

            json_match = re.search(
                r"```json\s*(\{.*?\})\s*```", response_content, re.DOTALL
            )
            if json_match:
                result = json.loads(json_match.group(1))
            else:
                raise ValueError("No valid JSON found in response")

        if not isinstance(result, dict):
            return {
                "has_sensitive_data": True,
                "analysis": "Invalid response format from Ollama",
                "leaked_fields": [],
            }

        if "has_sensitive_data" not in result:
            result["has_sensitive_data"] = True
        if "analysis" not in result:
            result["analysis"] = "No analysis provided"
        if "leaked_fields" not in result:
            result["leaked_fields"] = []

        return result

    except Exception as e:
        return {
            "has_sensitive_data": True,
            "analysis": f"Error during check: {str(e)}",
            "leaked_fields": [],
        }
    finally:
        if temp_image and os.path.exists(temp_image):
            os.remove(temp_image)


def check_sensitive_data(file_path):
    try:
        with open(file_path, "rb") as f:
            file_data = f.read()

        file_ext = Path(file_path).suffix.lower()
        mime_map = {
            ".jpg": "image/jpeg",
            ".jpeg": "image/jpeg",
            ".png": "image/png",
            ".pdf": "application/pdf",
        }
        mime_type = mime_map.get(file_ext, "image/jpeg")

        contents = [prompt, {"mime_type": mime_type, "data": file_data}]

        response = gemini_client.generate_content(contents=contents)

        try:
            result = json.loads(response.text)
        except json.JSONDecodeError:
            import re

            json_match = re.search(
                r"```json\s*(\{.*?\})\s*```", response.text, re.DOTALL
            )
            if json_match:
                result = json.loads(json_match.group(1))
            else:
                return {
                    "has_sensitive_data": True,
                    "analysis": f"Failed to parse Gemini response: {response.text[:200]}",
                    "leaked_fields": [],
                }

        if not isinstance(result, dict):
            return {
                "has_sensitive_data": True,
                "analysis": "Invalid response format from Gemini",
                "leaked_fields": [],
            }

        if "has_sensitive_data" not in result:
            result["has_sensitive_data"] = True
        if "analysis" not in result:
            result["analysis"] = "No analysis provided"
        if "leaked_fields" not in result:
            result["leaked_fields"] = []

        return result

    except Exception as e:
        return {
            "has_sensitive_data": True,
            "analysis": f"Error during check: {str(e)}",
            "leaked_fields": [],
        }


def process_files(input_dir, output_dir, use_ollama=False):
    check_function = check_sensitive_data_ollama if use_ollama else check_sensitive_data

    for root, _, files in os.walk(input_dir):
        for file in files:
            _, ext = os.path.splitext(file)
            if ext.lower() not in [
                ".png",
                ".jpg",
                ".jpeg",
                ".pdf",
            ]:
                continue

            file_path = os.path.join(root, file)

            rel_path = os.path.relpath(file_path, input_dir)

            print(f"guardrails üîç: validating '{rel_path}'")
            result = check_function(file_path)

            if result.get("has_sensitive_data", True):
                analysis = result.get("analysis", "No details provided")
                print(f"guardrails ‚ö†Ô∏è: '{rel_path}' sensitive data found - {analysis}")
            else:
                output_file_path = os.path.join(output_dir, rel_path)
                os.makedirs(os.path.dirname(output_file_path), exist_ok=True)
                shutil.move(file_path, output_file_path)
                print(
                    f"guardrails ‚úÖ: '{rel_path}' all data masked - {result['analysis']}"
                )


def main():
    parser = argparse.ArgumentParser(
        description="Validate masked files for sensitive data"
    )
    parser.add_argument(
        "-i",
        "--input",
        required=True,
        help="Directory containing masked files to validate",
    )
    parser.add_argument(
        "-o",
        "--output",
        required=True,
        help="Directory to copy files that passed validation",
    )
    parser.add_argument(
        "--ollama",
        action="store_true",
        help="use local model via Ollama instead of Gemini (better privacy)",
    )

    args = parser.parse_args()

    input_dir = os.path.abspath(args.input)
    output_dir = os.path.abspath(args.output)

    if not os.path.exists(input_dir):
        print(f"guardrails ‚ùå:  Input directory does not exist: {input_dir}")
        return

    if args.ollama:
        print("guardrails üîí: Using ollama (local) for validation")
    else:
        print("guardrails ‚òÅÔ∏è: Using Gemini for validation")

    os.makedirs(output_dir, exist_ok=True)

    process_files(input_dir, output_dir, args.ollama)
    remove_empty_dirs(input_dir)


if __name__ == "__main__":
    start_time = datetime.datetime.now()
    print(f"guardrails üöÄ: Starting guardrails validation at {start_time}")

    main()

    end_time = datetime.datetime.now()
    total_time = end_time - start_time
    print(f"guardrails ‚úÖ: Execution finished. Total time: {total_time}")
